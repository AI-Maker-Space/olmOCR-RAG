{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# D&D Rules RAG Pipeline with LangChain 1.0\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline using LangChain 1.0 to answer questions about D&D rules from our markdown documents.\n",
    "\n",
    "We will cover:\n",
    "1. **Environment Setup**: Loading API keys and configuring LangSmith tracing.\n",
    "2. **Indexing**: Loading documents, splitting them into chunks, and storing embeddings in Qdrant.\n",
    "3. **RAG Agent**: Building an agent with a retrieval tool for flexible Q&A.\n",
    "4. **RAG Chain**: A fast 2-step approach with a single LLM call per query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "We need to set our OpenAI API Key for embeddings and the LLM. We also enable LangSmith tracing for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}: \")\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "\n",
    "# LangSmith Tracing (optional but recommended)\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"D&D RAG Pipeline\"\n",
    "_set_if_undefined(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indexing-header",
   "metadata": {},
   "source": [
    "## 2. Indexing\n",
    "\n",
    "The indexing phase involves:\n",
    "1. **Loading** documents from our PDFs folder (markdown files)\n",
    "2. **Splitting** them into manageable chunks\n",
    "3. **Storing** embeddings in a Qdrant vector store\n",
    "\n",
    "### 2.1 Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Path to our markdown documents\n",
    "PDFS_DIR = Path(\"PDFs\")\n",
    "\n",
    "# Load all markdown files\n",
    "loader = DirectoryLoader(\n",
    "    str(PDFS_DIR),\n",
    "    glob=\"**/*.md\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Show document info\n",
    "for doc in docs:\n",
    "    print(f\"  - {Path(doc.metadata['source']).name}: {len(doc.page_content):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "splitting-header",
   "metadata": {},
   "source": [
    "### 2.2 Splitting Documents\n",
    "\n",
    "We use `RecursiveCharacterTextSplitter` to break documents into chunks that fit within model context windows and are retrievable individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(all_splits)} chunks\")\n",
    "\n",
    "# Preview a chunk\n",
    "print(f\"\\nExample chunk (first 500 chars):\\n{all_splits[0].page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "storing-header",
   "metadata": {},
   "source": [
    "### 2.3 Storing in Qdrant\n",
    "\n",
    "We use Qdrant as our vector store with OpenAI embeddings. Qdrant runs in-memory for this demo, but can be configured for persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-vectorstore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create in-memory Qdrant client\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Get embedding dimension\n",
    "vector_size = len(embeddings.embed_query(\"test\"))\n",
    "print(f\"Embedding dimension: {vector_size}\")\n",
    "\n",
    "# Create collection\n",
    "COLLECTION_NAME = \"dnd_rules\"\n",
    "\n",
    "if not client.collection_exists(COLLECTION_NAME):\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "    )\n",
    "    print(f\"Created collection: {COLLECTION_NAME}\")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "print(\"Adding documents to vector store...\")\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(f\"Added {len(document_ids)} document chunks to Qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-retrieval-header",
   "metadata": {},
   "source": [
    "### 2.4 Test Retrieval\n",
    "\n",
    "Let's verify our vector store is working by running a quick similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"What are the warforged racial traits?\"\n",
    "results = vector_store.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    source = Path(doc.metadata.get('source', 'Unknown')).name\n",
    "    print(f\"Result {i} (from {source}):\")\n",
    "    print(f\"{doc.page_content[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-agent-header",
   "metadata": {},
   "source": [
    "## 3. RAG Agent\n",
    "\n",
    "The **RAG Agent** approach uses LangChain 1.0's `create_agent` to build an agent that can decide when and how to retrieve information. This is flexible and can handle multi-step queries.\n",
    "\n",
    "### 3.1 Create Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-tool",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_dnd_rules(query: str):\n",
    "    \"\"\"Retrieve information from D&D rulebooks to help answer questions about rules, races, classes, spells, and game mechanics.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=4)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"Source: {Path(doc.metadata.get('source', 'Unknown')).name}\\nContent: {doc.page_content}\"\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "tools = [retrieve_dnd_rules]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-agent-header",
   "metadata": {},
   "source": [
    "### 3.2 Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the model\n",
    "model = init_chat_model(\"gpt-4o-mini\")\n",
    "\n",
    "# System prompt for the agent\n",
    "system_prompt = \"\"\"\n",
    "You are an expert Dungeon Master assistant with access to D&D rulebooks.\n",
    "Use the retrieve_dnd_rules tool to look up rules, racial traits, class features, \n",
    "spells, and other game mechanics when answering questions.\n",
    "\n",
    "Always cite which rulebook the information comes from.\n",
    "If you're unsure about something, say so rather than making up rules.\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt,\n",
    ")\n",
    "\n",
    "print(\"RAG Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent-header",
   "metadata": {},
   "source": [
    "### 3.3 Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a D&D question\n",
    "query = \"What are the racial traits of Warforged?\"\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-agent-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a more complex query requiring multiple lookups\n",
    "complex_query = \"\"\"\n",
    "I want to play a shifter character. What subraces are available, \n",
    "and what are their shifting features?\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Question: {complex_query}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": complex_query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-chain-header",
   "metadata": {},
   "source": [
    "## 4. RAG Chain (2-Step Approach)\n",
    "\n",
    "The **RAG Chain** is a faster, simpler approach that always retrieves context and uses a single LLM call. This is ideal for straightforward Q&A where you always want to search.\n",
    "\n",
    "| Approach | LLM Calls | Flexibility | Best For |\n",
    "|----------|-----------|-------------|----------|\n",
    "| **RAG Agent** | 2+ | High | Complex, multi-step queries |\n",
    "| **RAG Chain** | 1 | Low | Fast, simple Q&A |\n",
    "\n",
    "### 4.1 Create the RAG Chain with Middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents import AgentState, create_agent\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "\n",
    "# Extended state to store retrieved documents\n",
    "class RAGState(AgentState):\n",
    "    context: list[Document]\n",
    "\n",
    "# Middleware that retrieves and injects context\n",
    "class RetrieveContextMiddleware(AgentMiddleware[RAGState]):\n",
    "    state_schema = RAGState\n",
    "    \n",
    "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        \"\"\"Retrieve context before the model runs.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        \n",
    "        # Get the query text\n",
    "        query = last_message.content if hasattr(last_message, 'content') else str(last_message)\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = vector_store.similarity_search(query, k=4)\n",
    "        \n",
    "        # Format context\n",
    "        docs_content = \"\\n\\n---\\n\\n\".join(\n",
    "            f\"**Source: {Path(doc.metadata.get('source', 'Unknown')).name}**\\n{doc.page_content}\"\n",
    "            for doc in retrieved_docs\n",
    "        )\n",
    "        \n",
    "        # Create augmented message with context\n",
    "        augmented_content = (\n",
    "            f\"{query}\\n\\n\"\n",
    "            f\"---\\n\"\n",
    "            f\"Use the following context from D&D rulebooks to answer:\\n\\n\"\n",
    "            f\"{docs_content}\"\n",
    "        )\n",
    "        \n",
    "        # Return updated state\n",
    "        return {\n",
    "            \"messages\": [last_message.model_copy(update={\"content\": augmented_content})],\n",
    "            \"context\": retrieved_docs,\n",
    "        }\n",
    "\n",
    "# Create the RAG chain (agent with no tools, just middleware)\n",
    "rag_chain = create_agent(\n",
    "    model=model,\n",
    "    tools=[],  # No tools - context is injected via middleware\n",
    "    middleware=[RetrieveContextMiddleware()],\n",
    "    system_prompt=\"\"\"\n",
    "You are an expert Dungeon Master assistant. Answer questions about D&D rules \n",
    "based on the context provided. Always cite which rulebook the information comes from.\n",
    "If the context doesn't contain the answer, say so.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "print(\"RAG Chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-chain-header",
   "metadata": {},
   "source": [
    "### 4.2 Test the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG chain\n",
    "query = \"What is the Artificer wizard tradition?\"\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in rag_chain.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-chain-another",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another test\n",
    "query = \"How do dragonmarks work in Eberron?\"\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in rag_chain.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-header",
   "metadata": {},
   "source": [
    "## 5. Interactive Demo\n",
    "\n",
    "Use this cell to ask your own questions about D&D rules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_dnd_question(question: str, use_agent: bool = True):\n",
    "    \"\"\"\n",
    "    Ask a question about D&D rules.\n",
    "    \n",
    "    Args:\n",
    "        question: Your D&D rules question\n",
    "        use_agent: True for RAG Agent (flexible), False for RAG Chain (fast)\n",
    "    \"\"\"\n",
    "    print(f\"ðŸŽ² Question: {question}\")\n",
    "    print(f\"ðŸ“š Using: {'RAG Agent' if use_agent else 'RAG Chain'}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    pipeline = agent if use_agent else rag_chain\n",
    "    \n",
    "    response = pipeline.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": question}]}\n",
    "    )\n",
    "    \n",
    "    print(response[\"messages\"][-1].content)\n",
    "\n",
    "# Example usage:\n",
    "ask_dnd_question(\"What ability score increases do changelings get?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try-your-own",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question!\n",
    "ask_dnd_question(\"How does the shifting ability work for shifters?\", use_agent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built a complete RAG pipeline for D&D rules using LangChain 1.0:\n",
    "\n",
    "1. **Indexed** 3 D&D rulebook documents (markdown format) into Qdrant vector store\n",
    "2. **Created a RAG Agent** with a retrieval tool for flexible, multi-step queries\n",
    "3. **Created a RAG Chain** with middleware for fast, single-call Q&A\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| `DirectoryLoader` | Load markdown files from disk |\n",
    "| `RecursiveCharacterTextSplitter` | Split documents into retrievable chunks |\n",
    "| `QdrantVectorStore` | Store and search embeddings |\n",
    "| `@tool` decorator | Create retrieval tool for agent |\n",
    "| `create_agent` | Build LangChain 1.0 agent |\n",
    "| `AgentMiddleware` | Inject context for RAG chain |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Add conversational memory for multi-turn interactions\n",
    "- Deploy with LangServe for API access\n",
    "- Add more rulebooks to the knowledge base\n",
    "- Implement query rewriting for better retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
