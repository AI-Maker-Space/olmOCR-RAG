{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b35659a",
   "metadata": {},
   "source": [
    "# LangChain 1.0: Agents, HITL, and Multi-Agent Systems\n",
    "\n",
    "Welcome to this guide on leveraging LangChain 1.0! In this notebook, we'll explore the modern way to build agents using the `create_agent` workflow.\n",
    "\n",
    "We will cover:\n",
    "1. **Simple Agents**: Building a basic agent with tools.\n",
    "2. **Human-in-the-Loop (HITL)**: Adding oversight to our agents.\n",
    "3. **Retrieval**: Giving our agent access to external data (RAG) using OpenAI embeddings and Qdrant.\n",
    "4. **Multi-Agent Systems**: Composing multiple agents together.\n",
    "\n",
    "All the core logic is mirrored in the `src/` directory, demonstrating how to structure your project for deployment with LangSmith.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce80abc",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "Let's get our environment ready by `uv sync`ing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de28e86",
   "metadata": {},
   "source": [
    "## 2. Environment Variables\n",
    "We need to set our OpenAI API Key. We also enable LangSmith tracing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06d1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "\n",
    "# LangSmith Tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"LangChain 1.0 Guide\"\n",
    "_set_if_undefined(\"LANGSMITH_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90035553",
   "metadata": {},
   "source": [
    "## 3. The `create_agent` Workflow\n",
    "\n",
    "LangChain 1.0 introduces `create_agent` as the standard way to build agents. It simplifies the process while retaining the power of LangGraph under the hood.\n",
    "\n",
    "We've defined a simple agent in `src/agent.py` that has access to a weather tool and a \"magic calculator\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495dee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/chris/Code/AI Makerspace/Events/LangChain1.0/.venv/lib/python3.13/site-packages/pydantic/v1/main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco is currently rainy.\n"
     ]
    }
   ],
   "source": [
    "from src.agent import build_simple_agent\n",
    "\n",
    "# Build the agent\n",
    "agent = build_simple_agent()\n",
    "\n",
    "# Run the agent\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in San Francisco?\"}]})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef037f",
   "metadata": {},
   "source": [
    "## 4. Human-in-the-Loop (HITL)\n",
    "\n",
    "Sometimes we want to approve sensitive actions before they happen. LangChain 1.0 makes this easy with middleware.\n",
    "\n",
    "We've configured our `hitl_agent` to interrupt before using the `magic_calculator` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f20e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Asking to calculate (Thread: 019a9d46-3d3b-7704-a29a-d553acb69ee3) ---\n"
     ]
    }
   ],
   "source": [
    "from src.agent import build_hitl_agent\n",
    "from langgraph.types import Command\n",
    "# Import uuid7 for generating valid thread IDs compliant with LangSmith\n",
    "from langsmith import uuid7\n",
    "\n",
    "# Build the HITL agent\n",
    "hitl_agent = build_hitl_agent()\n",
    "\n",
    "# Configuration for the thread (required for checkpointing)\n",
    "# Using uuid7 to avoid warnings and ensure best practice\n",
    "thread_id = str(uuid7())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "print(f\"--- Asking to calculate (Thread: {thread_id}) ---\")\n",
    "# We use .stream() to see steps\n",
    "events = list(hitl_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Please use the magic calculator to add 5 and 5.\"}]},\n",
    "    config=config\n",
    "))\n",
    "\n",
    "# Print events to verify the initial run and interruption\n",
    "for i, event in enumerate(events):\n",
    "    # print(f\"Event {i} keys: {event.keys()}\")\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59098c56",
   "metadata": {},
   "source": [
    "You should see the tool call request, and then the stream ends (due to interruption). We can now resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60000d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found interrupt: a1ca32f1062ebba96b965ab156c80e2e\n",
      "--- Resuming (Thread: 019a9d46-3d3b-7704-a29a-d553acb69ee3) ---\n",
      "{'HumanInTheLoopMiddleware.after_model': {'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 112, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b1442291a8', 'id': 'chatcmpl-CdglVpeNLfx6QKf0NOQMX88qIBN2c', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--03f19c29-5a17-4c52-9105-c9143b85a048-0', tool_calls=[{'name': 'magic_calculator', 'args': {'a': 5, 'b': 5}, 'id': 'call_jdj09EAqk9Q15zNTBIdnadOF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 112, 'output_tokens': 19, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='20', name='magic_calculator', id='d29ea069-7e75-425a-a497-4340b3ea1ba2', tool_call_id='call_jdj09EAqk9Q15zNTBIdnadOF')]}}\n",
      "{'model': {'messages': [AIMessage(content='The result of adding 5 and 5, then multiplying by a random magic factor, is 20.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 141, 'total_tokens': 164, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b1442291a8', 'id': 'chatcmpl-CdglYJOGV3pFHlvUntOlnAUSdw09S', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--f269fdfc-926d-48e0-881b-5504d2990c66-0', usage_metadata={'input_tokens': 141, 'output_tokens': 23, 'total_tokens': 164, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "{'HumanInTheLoopMiddleware.after_model': None}\n"
     ]
    }
   ],
   "source": [
    "# Resume execution\n",
    "# We need to construct the resume payload dynamically based on the interrupt ID.\n",
    "\n",
    "# 1. Fetch the current state\n",
    "state = hitl_agent.get_state(config)\n",
    "\n",
    "# 2. Find the interrupt\n",
    "tasks = state.tasks\n",
    "resume_payload = {}\n",
    "\n",
    "if tasks and tasks[0].interrupts:\n",
    "    interrupt = tasks[0].interrupts[0]\n",
    "    print(f\"Found interrupt: {interrupt.id}\")\n",
    "    \n",
    "    # 3. Construct payload mapping interrupt ID to decision\n",
    "    resume_payload = {\n",
    "        interrupt.id: {\n",
    "            \"decisions\": [{\"type\": \"approve\"}]\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    print(\"No active interrupts found. Agent might have finished.\")\n",
    "\n",
    "# 4. Resume if we have a payload\n",
    "if resume_payload:\n",
    "    print(f\"--- Resuming (Thread: {thread_id}) ---\")\n",
    "    resume_output = list(hitl_agent.stream(\n",
    "        Command(resume=resume_payload),\n",
    "        config=config\n",
    "    ))\n",
    "    \n",
    "    if not resume_output:\n",
    "        print(\"No events received after resume. Checking final state...\")\n",
    "        final_state = hitl_agent.get_state(config)\n",
    "        if final_state.values and \"messages\" in final_state.values:\n",
    "             final_state.values[\"messages\"][-1].pretty_print()\n",
    "    else:\n",
    "        for event in resume_output:\n",
    "            if \"messages\" in event:\n",
    "                event[\"messages\"][-1].pretty_print()\n",
    "            else:\n",
    "                print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb422956",
   "metadata": {},
   "source": [
    "## 5. Retrieval (RAG)\n",
    "\n",
    "Agents become truly powerful when they can access external knowledge. We can add a retriever as a tool.\n",
    "\n",
    "We've set up a RAG pipeline over the \"Musk v Altman\" complaint in `src/rag.py`, using OpenAI embeddings and Qdrant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13626c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find specific information on when to use LangChain 1.0. However, LangChain is generally used for building applications with language models, and version 1.0 likely includes updates and features that enhance its capabilities. If you have specific requirements or features in mind, I can help you find more detailed information.\n"
     ]
    }
   ],
   "source": [
    "from src.agent import build_rag_agent\n",
    "\n",
    "rag_agent = build_rag_agent()\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"When should I use LangChain 1.0?\"}]\n",
    "})\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c8658",
   "metadata": {},
   "source": [
    "## 6. Multi-Agent Systems\n",
    "\n",
    "LangChain 1.0 agents are graphs, which means they can be composed! We can treat an agent as a tool for another agent.\n",
    "\n",
    "We have a \"Supervisor\" that delegates to a \"Researcher\" (who has RAG access) and a \"Writer\" (who has a specific persona).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef276a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Shakespearean sonnet about when to use LangChain versus LangGraph:\n",
      "\n",
      "When language flows like rivers swift and bright,  \n",
      "And words must dance upon the tongue with grace,  \n",
      "Then LangChain's art shall guide thee through the night,  \n",
      "To craft a voice that time cannot erase.  \n",
      "\n",
      "Yet when the mind seeks webs of thought profound,  \n",
      "Where data's threads in complex patterns weave,  \n",
      "LangGraph shall rise, its structured paths unbound,  \n",
      "To map the truths that hidden depths conceive.  \n",
      "\n",
      "LangChain, the bard of chat and speech refined,  \n",
      "In simple verse, it speaks to hearts and minds.  \n",
      "LangGraph, the sage of knowledge intertwined,  \n",
      "In graphs and nodes, the deeper meaning finds.  \n",
      "\n",
      "Choose LangChain for the art of language pure,  \n",
      "For graphs and depth, let LangGraph be your cure.\n"
     ]
    }
   ],
   "source": [
    "from src.agent import build_multi_agent_system\n",
    "\n",
    "supervisor = build_multi_agent_system()\n",
    "\n",
    "# This complex query requires research and then writing\n",
    "query = \"When should I use LangChain vs. LangGraph, write a Shakespearean sonnet about that.\"\n",
    "\n",
    "response = supervisor.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "})\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
